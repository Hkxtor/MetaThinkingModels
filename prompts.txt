Start a python project in this directory. 
The project is empowering LLMs with different thinking models for solving different real-life problems. Using this project, query handling takes 2 LLM calls: First, the query and the thinking models definitions are sent to LLM, for it to select relevant thinking models that can help with problem solving; Second, the query along with selected think models (definition/examples) are put into context to prompt the LLM. Hopefully, with the help of thinking models, LLMs can solve users' problems better.
The project does the following:
1. Parse all the files in models directory, and build a data structure in memory, indexed by each model's id.
2. Phase 1 of query handling: use a prompt template to wrap the query and all the thinking models definitions, asking LLM to return the relevant thinking model ids.
3. Phase 2 of query handling: get the selected models (definition and examples), wrap them and the query in a prompt template, asking LLM to solve the query, and use the thinking models as guidance if they are helpful.
4. Accept configuration of any OpenAI compatible LLM API. The API URL can be set with an environment variable.
5. Use a CLI interface to accept queries and present LLM returned result.
6. Develop a web server and web UI to accept user query and present LLM results.

Please:
1. draft a step-by-step development plan in a .md file, with todo items defined.
2. Start with the first step: model files parsing.

$env:LLM_API_URL="https://openrouter.ai/api/v1"
$env:LLM_API_KEY="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"
$env:LLM_MODEL_NAME="google/gemma-3-27b-it:free"